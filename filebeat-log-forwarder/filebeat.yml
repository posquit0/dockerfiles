######################## Filebeat Configuration ############################

#========================= Filebeat global options ============================
# Name of the registry file. If a relative path is used, it is considered relative to the
# data path.
filebeat.registry_file: ${path.data}/registry
#
# These config files must have the full filebeat config part inside, but only
# the prospector part is processed. All global options like spool_size are ignored.
# The config_dir MUST point to a different directory then where the main filebeat config file is in.
# filebeat.config_dir:

# How long filebeat waits on shutdown for the publisher to finish.
# Default is 0, not waiting.
filebeat.shutdown_timeout: 0

# Enable filebeat config reloading
filebeat.config:
  prospectors:
    enabled: false
    path: ${path.config}/prospectors.d/*.yml
    reload.enabled: true
    reload.period: 10s
  modules:
    enabled: false
    path: ${path.config}/modules.d/*.yml
    reload.enabled: true
    reload.period: 10s


#================================ General =====================================
# The name of the shipper that publishes the network data. It can be used to group
# all the transactions sent by a single shipper in the web interface.
name: ${FILEBEAT_NAME:}

# The tags of the shipper are included in their own field with each transaction published.
tags: '${FILEBEAT_TAGS:docker}'

# Optional fields that you can specify to add additional information to the output.
fields:
  env: ${FILEBEAT_ENV:development}

# If this option is set to true, the custom fields are stored as top-level
# fields in the output document instead of being grouped under a fields
# sub-dictionary. Default is false.
fields_under_root: true

# Internal queue configuration for buffering events to be published.
queue:
  # Queue type by name (default 'mem')
  # The memory queue will present all available events (up to the outputs
  # bulk_max_size) to the output, the moment the output is ready to server
  # another batch of events.
  mem:
    # Max number of events the queue can buffer.
    events: 4096

    # Hints the minimum number of events stored in the queue,
    # before providing a batch of events to the outputs.
    # A value of 0 (the default) ensures events are immediately available
    # to be sent to the outputs.
    flush.min_events: 0

    # Maximum duration after which events are available to the outputs,
    # if the number of events stored in the queue is < min_flush_events.
    flush.timeout: 0s

# Sets the maximum number of CPUs that can be executing simultaneously. The
# default is the number of logical CPUs available in the system.
# max_procs:


#================================= Paths ======================================
# The home path for the filebeat installation. This is the default base path
# for all other path settings and for miscellaneous files that come with the
# distribution (for example, the sample dashboards).
# If not set by a CLI flag or in the configuration file, the default for the
# home path is the location of the binary.
path.home: ${FILEBEAT_PATH_HOME:/usr/share/filebeat}

# The configuration path for the filebeat installation. This is the default
# base path for configuration files, including the main YAML configuration file
# and the Elasticsearch template file. If not set by a CLI flag or in the
# configuration file, the default for the configuration path is the home path.
path.config: ${path.home}

# The data path for the filebeat installation. This is the default base path
# for all the files in which filebeat needs to store its data. If not set by a
# CLI flag or in the configuration file, the default for the data path is a data
# subdirectory inside the home path.
path.data: ${path.home}/data

# The logs path for a filebeat installation. This is the default location for
# the Beat's log files. If not set by a CLI flag or in the configuration file,
# the default for the logs path is a logs subdirectory inside the home path.
path.logs: ${path.home}/logs


#================================ Logging =====================================
# Sets log level. The default log level is info.
# Available log levels are: critical, error, warning, info, debug
logging.level: ${FILEBEAT_LOGGING_LEVEL:warning}

# At debug level, you can selectively enable logging only for some components.
# To enable all selectors use ["*"]. Examples of other selectors are "beat", "publish", "service".
logging.selectors: ["*"]

# Send all logging output to syslog. The default is false.
logging.to_syslog: false

# Send all logging output to Windows Event Logs. The default is false.
logging.to_eventlog: false

# If enabled, filebeat periodically logs its internal metrics that have changed
# in the last period. For each metric that changed, the delta from the value at
# the beginning of the period is logged. Also, the total values for
# all non-zero internal metrics are logged on shutdown. The default is true.
logging.metrics.enabled: ${FILEBEAT_LOGGING_METRICS_ENABLED:true}

# The period after which to log the internal metrics. The default is 30s.
logging.metrics.period: 30s

# Set to true to log messages in json format.
logging.json: ${FILEBEAT_LOGGING_JSON:false}

# Logging to rotating files. Set logging.to_files to false to disable logging to files.
logging.to_files: ${FILEBEAT_LOGGING_ENABLED:true}

logging.files:
  # Configure the path where the logs are written. The default is the logs directory
  # under the home path (the binary location).
  path: ${path.logs}

  # The name of the files where the logs are written to.
  name: filebeat

  # Configure log file size limit. If limit is reached, log file will be automatically rotated (10MB = 10485760)
  rotateeverybytes: 10485760

  # Number of rotated log files to keep. Oldest files will be deleted first.
  keepfiles: 7

  # The permissions mask to apply when rotating log files. The default value is 0600.
  # Must be a valid Unix-style file permissions mask expressed in octal notation.
  permissions: 0600


#============================== Dashboards =====================================
# These settings control loading the sample dashboards to the Kibana index. Loading
# the dashboards is disabled by default and can be enabled either by setting the
# options here, or by using the `-setup` CLI flag or the `setup` command.
setup.dashboards.enabled: ${FILEBEAT_DASHBOARDS_ENABLED:false}

# The directory from where to read the dashboards. The default is the `kibana` folder in the home path.
setup.dashboards.directory: ${path.home}/kibana

# The URL from where to download the dashboards archive. It is used instead of the directory if it has a value.
# setup.dashboards.url:

# The file archive (zip file) from where to read the dashboards. It is used instead
# of the directory when it has a value.
# setup.dashboards.file:

# In case the archive contains the dashboards from multiple Beats, this lets you
# select which one to load. You can load all the dashboards in the archive by
# setting this to the empty string.
setup.dashboards.beat: filebeat

# The name of the Kibana index to use for setting the configuration. Default is ".kibana"
setup.dashboards.kibana_index: .kibana

# The Elasticsearch index name. This overwrites the index name defined in the
# dashboards and index pattern. Example: testbeat-*
# setup.dashboards.index:

# Always use the Kibana API for loading the dashboards instead of autodetecting
# how to install the dashboards by first querying Elasticsearch.
setup.dashboards.always_kibana: false


#============================== Template =====================================
# Set to false to disable template loading.
setup.template.enabled: ${FILEBEAT_TEMPLATE_ENABLED:false}

# Template name. By default the template name is "filebeat-%{[beat.version]}"
# The template name and pattern has to be set in case the elasticsearch index pattern is modified.
setup.template.name: "filebeat-%{[beat.version]}"

# Template pattern. By default the template pattern is "-%{[beat.version]}-*" to apply to the default index settings.
# The first part is the version of the beat and then -* is used to match all daily indices.
# The template name and pattern has to be set in case the elasticsearch index pattern is modified.
setup.template.pattern: "filebeat-%{[beat.version]}-*"

# Path to fields.yml file to generate the template
setup.template.fields: "${path.config}/fields.yml"

# Overwrite existing template
setup.template.overwrite: false

# Elasticsearch template settings
setup.template.settings:
  # A dictionary of settings to place into the settings.index dictionary
  # of the Elasticsearch template. For more details, please check
  # https://www.elastic.co/guide/en/elasticsearch/reference/current/mapping.html
  # index:
  #   number_of_shards: 1
  #   codec: best_compression
  #   number_of_routing_shards: 30

  # A dictionary of settings for the _source field. For more details, please check
  # https://www.elastic.co/guide/en/elasticsearch/reference/current/mapping-source-field.html
  # _source:
  #   enabled: false


#============================== Kibana =====================================
setup.kibana:
  # Scheme and port can be left out and will be set to the default (http and 5601)
  # In case you specify and additional path, the scheme is required: http://localhost:5601/path
  # IPv6 addresses should always be defined as: https://[2001:db8::1]:5601
  hosts: '${FILEBEAT_KIBANA_HOST:kibana.shared:5601}'

  # Optional protocol and basic auth credentials.
  # protocol: "https"
  # username: "elastic"
  # password: "changeme"

  # Optional HTTP Path
  # path: ""


#============================== Xpack Monitoring =====================================
# Set to true to enable the monitoring reporter.
xpack.monitoring.enabled: ${FILEBEAT_MONITORING_ENABLED:true}

# Uncomment to send the metrics to Elasticsearch. Most settings from the
# Elasticsearch output are accepted here as well. Any setting that is not set is
# automatically inherited from the Elasticsearch output configuration, so if you
# have the Elasticsearch output configured, you can simply uncomment the
# following line, and leave the rest commented out.
xpack.monitoring.elasticsearch:
  # Array of hosts to connect to.
  hosts: '${FILEBEAT_ELASTICSEARCH_HOSTS:elasticsearch.shared:9200}'

  # Set gzip compression level.
  compression_level: ${FILEBEAT_ELASTICSEARCH_COMPRESSION_LEVEL:0}

  # Optional protocol and basic auth credentials.
  # protocol: "https"
  # username: "elastic"
  # password: "changeme"

  # The number of times a particular Elasticsearch index operation is attempted. If
  # the indexing operation doesn't succeed after this many retries, the events are dropped. The default is 3.
  max_retries: ${FILEBEAT_ELASTICSEARCH_MAX_RETRIES:3}

  # The maximum number of events to bulk in a single Elasticsearch bulk API index request. The default is 50.
  bulk_max_size: ${FILEBEAT_ELASTICSEARCH_BULK_MAX_SIZE:50}

  # Configure http request timeout before failing an request to Elasticsearch.
  timeout: ${FILEBEAT_ELASTICSEARCH_TIMEOUT:60s}


#================================ HTTP Endpoint ======================================
# Defines if the HTTP endpoint is enabled.
http.enabled: false

# The HTTP endpoint will bind to this hostname or IP address. It is recommended to use only localhost.
http.host: localhost

# Port on which the HTTP endpoint will bind. Default is 5066.
http.port: 5066


#============================= Elastic Cloud ==================================
# The cloud.id setting overwrites the `output.elasticsearch.hosts` and `setup.kibana.host` options.
# You can find the `cloud.id` in the Elastic Cloud web UI.
# cloud.id:

# The cloud.auth setting overwrites the `output.elasticsearch.username` and
# `output.elasticsearch.password` settings. The format is `<user>:<pass>`.
# cloud.auth:


#================================ Processors ===================================
processors:
# Enriches each event with metadata from the cloud provider about the host machine.
# It works on EC2, GCE, DigitalOcean, Tencent Cloud, and Alibaba Cloud.
- add_cloud_metadata: ~

# Enriches each event with docker metadata, it matches given fields to an existing
# container id and adds info from that container:
# - add_docker_metadata:
    # Docker socket (UNIX or TCP socket). It uses unix:///var/run/docker.sock by default.
    # host: "unix:///var/run/docker.sock"
    # Time of inactivity to consider we can clean and forget metadata for a container, 60s by default.
    # cleanup_timeout: 60
    # Match container id from a log path present in source field. Enabled by default.
    # match_source: false
    # Index in the source path split by / to look for container id. It defaults to 4 to match
    # match_source_index: 4
    # A list of fields to match a container id, at least one of them should hold a container id to get the event enriched.
    # match_fields: ["system.process.cgroup.id", "hostname"]
    # To connect to Docker over TLS you must specify a client and CA certificate.
    # ssl:
    #   certificate_authority: "/etc/pki/root/ca.pem"
    #   certificate: "/etc/pki/client/cert.pem"
    #   key: "/etc/pki/client/cert.key"


#========================== Filebeat autodiscover ==============================
# filebeat.autodiscover:
#   providers:
#     - type: docker
#       templates:
#         - condition:
#             equals.docker.container.image: "*"
#           config:
#             - type: docker
#               containers.ids:
#                 - "${data.docker.container.id}"


#================================ Outputs =====================================
#-------------------------- Elasticsearch output ------------------------------
output.elasticsearch:
  # Boolean flag to enable or disable the output module.
  enabled: false

  # Array of hosts to connect to.
  hosts: '${FILEBEAT_ELASTICSEARCH_HOSTS:elasticsearch.shared:9200}'

  # Set gzip compression level.
  compression_level: ${FILEBEAT_ELASTICSEARCH_COMPRESSION_LEVEL:0}

  # Optional protocol and basic auth credentials.
  # protocol: "https"
  # username: "elastic"
  # password: "changeme"

  # The number of times a particular Elasticsearch index operation is attempted. If
  # the indexing operation doesn't succeed after this many retries, the events are dropped. The default is 3.
  max_retries: ${FILEBEAT_ELASTICSEARCH_MAX_RETRIES:3}

  # The maximum number of events to bulk in a single Elasticsearch bulk API index request. The default is 50.
  bulk_max_size: ${FILEBEAT_ELASTICSEARCH_BULK_MAX_SIZE:50}

  # Configure http request timeout before failing an request to Elasticsearch.
  timeout: ${FILEBEAT_ELASTICSEARCH_TIMEOUT:60s}

#----------------------------- Logstash output --------------------------------
output.logstash:
  # Boolean flag to enable or disable the output module.
  enabled: true

  # The Logstash hosts
  hosts: '${FILEBEAT_LOGSTASH_HOSTS:logstash.shared:5044}'

  # Number of workers per Logstash host.
  worker: ${FILEBEAT_LOGSTASH_WORKER:1}

  # Set gzip compression level.
  compression_level: ${FILEBEAT_LOGSTASH_COMPRESSION_LEVEL:3}

  # Optional load balance the events between the Logstash hosts. Default is false.
  loadbalance: ${FILEBEAT_LOGSTASH_LOADBALANCE:true}

  # Optional index name. The default index name is set to filebeat
  # in all lowercase.
  index: ${FILEBEAT_LOGSTASH_INDEX:filebeat}

  # The number of seconds to wait for responses from the Logstash server before timing out. The default is 30 (seconds).
  timeout: ${FILEBEAT_LOGSTASH_TIMEOUT:30s}

  # The number of times to retry publishing an event after a publishing failure. After the specified number of retries, the events are typically dropped. Some Beats, such as Filebeat, ignore the max_retries setting and retry until all events are published.
  # Set max_retries to a value less than 0 to retry until all events are published.
  # Default is 3.
  max_retries: ${FILEBEAT_LOGSTASH_MAX_RETRIES:5}

  # Optional SSL. By default is off.
  # List of root certificates for HTTPS server verifications
  # ssl.certificate_authorities: ["/etc/pki/root/ca.pem"]

  # Certificate for SSL client authentication
  # ssl.certificate: "/etc/pki/client/cert.pem"

  # Client Certificate Key
  # ssl.key: "/etc/pki/client/cert.key"

#------------------------------- File output -----------------------------------
output.file:
  # Boolean flag to enable or disable the output module.
  enabled: false

  # Path to the directory where to save the generated files. The option is mandatory.
  path: "/tmp/filebeat"

  # Name of the generated files. The default is `filebeat` and it generates
  # files: `filebeat`, `filebeat.1`, `filebeat.2`, etc.
  filename: filebeat

  # Maximum size in kilobytes of each file. When this size is reached, and on
  # every filebeat restart, the files are rotated. The default value is 10240 kB.
  rotate_every_kb: 10000

  # Maximum number of files under path. When this number of files is reached,
  # the oldest file is deleted and the rest are shifted from last to first. The
  # default is 7 files.
  number_of_files: 7

  # Permissions to use for file creation. The default is 0600.
  permissions: 0600

#----------------------------- Console output ---------------------------------
output.console:
  # Boolean flag to enable or disable the output module.
  enabled: false

  # Pretty print json event
  pretty: true


#=========================== Filebeat prospectors =============================
filebeat.prospectors:
- type: log
  # Change to true to enable this prospector configuration.
  enabled: true
  # Paths that should be crawled and fetched. Glob based paths.
  paths:
    - ${FILEBEAT_APP_LOG_DIRECTORY:/log}/*.log*

  # Exclude lines. A list of regular expressions to match. It drops the lines that are
  # matching any regular expression from the list.
  # exclude_lines: ['^DBG']

  # Include lines. A list of regular expressions to match. It exports the lines that are
  # matching any regular expression from the list.
  # include_lines: ['^ERR', '^WARN']

  # Exclude files. A list of regular expressions to match. Filebeat drops the files that
  # are matching any regular expression from the list. By default, no files are dropped.
  # exclude_files: ['.gz$']

  # Optional additional fields. These fields can be freely picked
  # to add additional information to the crawled log files for filtering
  # fields:
  #   level: debug
  #   review: 1

  ### Recursive glob configuration
  # Expand "**" patterns into regular glob patterns.
  # recursive_glob.enabled: true

  ### JSON configuration
  # Decode JSON options. Enable this if your logs are structured in JSON.
  # JSON key on which to apply the line filtering and multiline settings. This key
  # must be top level and its value must be string, otherwise it is ignored. If
  # no text key is defined, the line filtering and multiline features cannot be used.
  # json.message_key: message

  # By default, the decoded JSON is placed under a "json" key in the output document.
  # If you enable this setting, the keys are copied top level in the output document.
  json.keys_under_root: true

  # If keys_under_root and this setting are enabled, then the values from the decoded
  # JSON object overwrite the fields that Filebeat normally adds (type, source, offset, etc.)
  # in case of conflicts.
  json.overwrite_keys: false

  # If this setting is enabled, Filebeat adds a "error.message" and "error.key: json" key in case of JSON
  # unmarshaling errors or when a text key is defined in the configuration but cannot
  # be used.
  json.add_error_key: true

  ### Multiline options
  # The regexp Pattern that has to be matched. The example pattern matches all lines starting with [
  # multiline.pattern: ^\[

  # Defines if the pattern set under pattern should be negated or not. Default is false.
  # multiline.negate: false

  # Match can be set to "after" or "before". It is used to define if lines should be append to a pattern
  # that was (not) matched before or after or as long as a pattern is not matched based on negate.
  # Note: After is the equivalent to previous and before is the equivalent to to next in Logstash
  # multiline.match: after

  ### Harvester closing options
  # Close inactive closes the file handler after the predefined period.
  # The period starts when the last line of the file was, not the file ModTime.
  # Time strings like 2h (2 hours), 5m (5 minutes) can be used.
  # close_inactive: 12h

  # Close renamed closes a file handler when the file is renamed or rotated.
  # Note: Potential data loss. Make sure to read and understand the docs for this option.
  close_renamed: false

  # When enabling this option, a file handler is closed immediately in case a file can't be found
  # any more. In case the file shows up again later, harvesting will continue at the last known position
  # after scan_frequency.
  close_removed: true

  # Closes the file handler as soon as the harvesters reaches the end of the file.
  # By default this option is disabled.
  # Note: Potential data loss. Make sure to read and understand the docs for this option.
  close_eof: false

  ### State options
  # Files for the modification data is older then clean_inactive the state from the registry is removed
  # By default this is disabled.
  # clean_inactive: 0

  # Removes the state for file which cannot be found on disk anymore immediately
  clean_removed: true

  # Close timeout closes the harvester after the predefined time.
  # This is independent if the harvester did finish reading the file or not.
  # By default this option is disabled.
  # Note: Potential data loss. Make sure to read and understand the docs for this option.
  # close_timeout: 0
